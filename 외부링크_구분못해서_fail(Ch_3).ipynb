{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "외부링크 구분못해서 fail(Ch.3)",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMJN2EK15/kAWDB9bMkux7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttury/Web-Scraping-with-Python/blob/main/%EC%99%B8%EB%B6%80%EB%A7%81%ED%81%AC_%EA%B5%AC%EB%B6%84%EB%AA%BB%ED%95%B4%EC%84%9C_fail(Ch_3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbky1U_l7cvN"
      },
      "source": [
        "from urllib.request import urlopen\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "html = urlopen(\"https://en.wikipedia.org/wiki/Dream\")\r\n",
        "bs = BeautifulSoup(html, \"html.parser\")\r\n",
        "for link in bs.findAll(\"a\"):\r\n",
        "  if \"href\" in link.attrs:\r\n",
        "    print(link.attrs[\"href\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNIj_VSF_-QU"
      },
      "source": [
        "from urllib.request import urlopen\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import re\r\n",
        "\r\n",
        "html = urlopen(\"https://en.wikipedia.org/wiki/Kevin_Bacon\")\r\n",
        "bs = BeautifulSoup(html, \"html.parser\")\r\n",
        "for link in bs.find(\"div\", {\"id\":\"bodyContent\"}).findAll('a', href = re.compile('^(/wiki/)((?!:).)*$')):\r\n",
        "  if 'href' in link.attrs:\r\n",
        "    print(link.attrs[\"href\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYHHN9lzF6R4"
      },
      "source": [
        "# 케빈 베이컨 문서 내 항목 링크 중 랜덤으로 선택해 이동\r\n",
        "\r\n",
        "from urllib.request import urlopen\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import datetime\r\n",
        "import random\r\n",
        "import re\r\n",
        "\r\n",
        "random.seed(datetime.datetime.now())\r\n",
        "\r\n",
        "def getLinks(articleUrl):\r\n",
        "  html = urlopen(\"http://en.wikipedia.org{}\".format(articleUrl))\r\n",
        "  bs = BeautifulSoup(html, \"html.parser\")\r\n",
        "  return bs.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\", href = re.compile(\"^(/wiki/)((?!:).)*$\"))\r\n",
        "\r\n",
        "links = getLinks('/wiki/Kevin_Bacon')\r\n",
        "while len(links) > 0:\r\n",
        "  newArticle = links[random.randint(0, len(links) - 1)].attrs['href']\r\n",
        "  print(newArticle)\r\n",
        "  links = getLinks(newArticle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6Ng1La3LVzF"
      },
      "source": [
        "# 예외 처리 버전\r\n",
        "\r\n",
        "from urllib.request import urlopen\r\n",
        "from urllib.request import HTTPError\r\n",
        "from urllib.request import URLError\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import re\r\n",
        "import datetime\r\n",
        "import random\r\n",
        "\r\n",
        "random.seed(datetime.datetime.now())\r\n",
        "\r\n",
        "def getLinks(articleUrl):\r\n",
        "  try:\r\n",
        "    html = urlopen('https://en.wikipedia.org{}'.format(articleUrl))\r\n",
        "  except HTTPError as e:\r\n",
        "    return None\r\n",
        "  except URLError as e:\r\n",
        "    return None\r\n",
        "  try:\r\n",
        "    bs = BeautifulSoup(html, 'html.parser')\r\n",
        "  except AttributeError as e:\r\n",
        "    return None\r\n",
        "  return bs.find('div', {'id' : 'bodyContent'}).findAll('a', href = re.compile('^(/wiki/)((?!:).)*$'))\r\n",
        "\r\n",
        "links = getLinks('/wiki/Kevin_Bacon')\r\n",
        "while len(links) > 0 and links != None:\r\n",
        "  newArticle = links[random.randint(0, len(links) - 1)].attrs['href']\r\n",
        "  print(newArticle)\r\n",
        "  links = getLinks(newArticle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO-ounbmGBUA"
      },
      "source": [
        "# 위키백과 메인 페이지 내 다른 링크로 이동(중복 제외)\r\n",
        "\r\n",
        "from urllib.request import urlopen\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import re\r\n",
        "\r\n",
        "pages = set()\r\n",
        "def getLinks(pageUrl):\r\n",
        "  global pages\r\n",
        "  html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))\r\n",
        "  bs = BeautifulSoup(html, 'html.parser')\r\n",
        "  for link in bs.findAll('a', href = re.compile('^(/wiki/)')):\r\n",
        "    if 'href' in link.attrs:\r\n",
        "      if link.attrs['href'] not in pages: #새 페이지 발견\r\n",
        "        newPage = link.attrs['href']\r\n",
        "        print(newPage)\r\n",
        "        pages.add(newPage)\r\n",
        "        getLinks(newPage)\r\n",
        "\r\n",
        "getLinks('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lw-dn4FPiTB"
      },
      "source": [
        "from urllib.request import urlopen\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import re\r\n",
        "\r\n",
        "pages = set() # set을 이용한 중복 탐색 페이지 탐지\r\n",
        "def getLinks(pageUrl):\r\n",
        "  global pages\r\n",
        "  html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))\r\n",
        "  bs = BeautifulSoup(html, 'html.parser')\r\n",
        "\r\n",
        "# 정보 출력\r\n",
        "  try:\r\n",
        "    print(bs.h1.get_text()) # 제목 출력\r\n",
        "    print(bs.find(id = 'mw-content-text').find('p').get_text()) # 첫 번째 문단 출력\r\n",
        "    print(bs.find(id = 'ca-edit').find('a').attrs['href']) # 편집 페이지 출력\r\n",
        "  except AttributeError:\r\n",
        "    print('This page is missing something! No Worries though!') # 제목 -> 첫문단 -> 편집 순의 우선순위로 AttributeError 탐지(존재 여부)\r\n",
        "\r\n",
        "# 다음 페이지 탐색\r\n",
        "  for link in bs.findAll('a', href = re.compile('^(/wiki/)')):\r\n",
        "    if 'href' in link.attrs:\r\n",
        "      if link.attrs['href'] not in pages:\r\n",
        "        newPage = link.attrs['href']\r\n",
        "        pages.add(newPage)\r\n",
        "        print('---------------------------\\n' + newPage)\r\n",
        "        getLinks(newPage)\r\n",
        "\r\n",
        "getLinks('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fiuxek9sUA6J"
      },
      "source": [
        "from urllib.request import urlopen\r\n",
        "from urllib.parse import urlparse\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import re\r\n",
        "import datetime\r\n",
        "import random\r\n",
        "\r\n",
        "pages = set()\r\n",
        "random.seed(datetime.datetime.now())\r\n",
        "\r\n",
        "def getInternalLinks(bs, includeUrl):\r\n",
        "  includeUrl = '{}://{}'.format(urlparse(includeUrl).scheme, urlparse(includeUrl).netloc)\r\n",
        "  internalLinks = []\r\n",
        "  for link in bs.findAll('a', href = re.compile('^(/|.*' + includeUrl + ')')):\r\n",
        "    if link.attrs['href'] is not None:\r\n",
        "      if link.attrs['href'] not in internalLinks:\r\n",
        "        if link.attrs['href'].startswith('/'):\r\n",
        "          internalLinks.append(includeUrl + link.attrs['href'])\r\n",
        "        else:\r\n",
        "          internalLinks.append(link.attrs['href'])\r\n",
        "  return internalLinks\r\n",
        "  \r\n",
        "\r\n",
        "def getExternalLinks(bs, excludeUrl):\r\n",
        "  externalLinks = []\r\n",
        "  for link in bs.findAll('a', href = re.compile('^(https|www)((?!' + excludeUrl + ').)*$')):\r\n",
        "    if link.attrs['href'] is not None:\r\n",
        "      if link.attrs['href'] not in externalLinks:\r\n",
        "        externalLinks.append(link.attrs['href'])\r\n",
        "  return externalLinks\r\n",
        "\r\n",
        "def getRandomExternalLink(startingPage):\r\n",
        "  html = urlopen(startingPage)\r\n",
        "  bs = BeautifulSoup(html, 'html.parser')\r\n",
        "  externalLinks = getExternalLinks(bs, urlparse(startingPage).netloc)\r\n",
        "\r\n",
        "  if len(externalLinks) == 0:\r\n",
        "    print('No external links, looking around the site for one')\r\n",
        "    domain = '{}://{}'.format(urlparse(startingPage).scheme, urlparse(startingPage).netloc)\r\n",
        "    internalLinks = getInternalLinks(bs, domain)\r\n",
        "    return getRandomExternalLink(internalLinks[random.randint(0, len(internalLinks) - 1)])\r\n",
        "\r\n",
        "  else:\r\n",
        "    return externalLinks[random.randint(0, len(externalLinks) - 1)]\r\n",
        "\r\n",
        "def followExternalOnly(startingPage):\r\n",
        "  externalLink = getRandomExternalLink(startingPage)\r\n",
        "  print('random external link is: {}'.format(externalLink))\r\n",
        "  followExternalOnly(externalLink)\r\n",
        "\r\n",
        "followExternalOnly('https://okky.kr/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42_yGgk8EEd6"
      },
      "source": [
        "from urllib.request import urlopen\r\n",
        "from urllib.parse import urlparse\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import re\r\n",
        "import datetime\r\n",
        "import random\r\n",
        "\r\n",
        "random.seed(datetime.datetime.now())\r\n",
        "\r\n",
        "allExtLinks = set()\r\n",
        "allIntLinks = set()\r\n",
        "\r\n",
        "def getInternalLinks(bs, includeUrl):\r\n",
        "  includeUrl = '{}://{}'.format(urlparse(includeUrl).scheme, urlparse(includeUrl).netloc)\r\n",
        "  internalLinks = []\r\n",
        "  for link in bs.findAll('a', href = re.compile('^(/|.*' + includeUrl + ')')):\r\n",
        "    if link.attrs['href'] is not None:\r\n",
        "      if link.attrs['href'] not in internalLinks:\r\n",
        "        if link.attrs['href'].startswith('/'):\r\n",
        "          internalLinks.append(includeUrl + link.attrs['href'])\r\n",
        "        else:\r\n",
        "          internalLinks.append(link.attrs['href'])\r\n",
        "  return internalLinks\r\n",
        "  \r\n",
        "\r\n",
        "def getExternalLinks(bs, excludeUrl):\r\n",
        "  externalLinks = []\r\n",
        "  for link in bs.findAll('a', href = re.compile('(?=^(https|www))(?=((?!' + excludeUrl + ').)*$)')):\r\n",
        "    if link.attrs['href'] is not None:\r\n",
        "      if link.attrs['href'] not in externalLinks:\r\n",
        "        externalLinks.append(link.attrs['href'])\r\n",
        "  return externalLinks\r\n",
        "\r\n",
        "def findAllExtLinksInThisSite(siteUrl):\r\n",
        "  html = urlopen(siteUrl)\r\n",
        "  domain = '{}://{}'.format(urlparse(siteUrl).scheme, urlparse(siteUrl).netloc)\r\n",
        "  bs = BeautifulSoup(html, 'html.parser')\r\n",
        "  internalLinks = getInternalLinks(bs, domain)\r\n",
        "  externalLinks = getExternalLinks(bs, domain)\r\n",
        "  print(externalLinks)\r\n",
        "\r\n",
        "  for link in externalLinks:\r\n",
        "    if link not in allExtLinks:\r\n",
        "      allExtLinks.add(link)\r\n",
        "      print(link)\r\n",
        "  \r\n",
        "  for link in internalLinks:\r\n",
        "    if link not in allIntLinks:\r\n",
        "      allIntLinks.add(link)\r\n",
        "      findAllExtLinksInThisSite(link)\r\n",
        "\r\n",
        "allIntLinks.add('https://www.oreilly.com')\r\n",
        "findAllExtLinksInThisSite('https://www.oreilly.com')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}